{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load YAMNet Model and Class Labels ----\n",
    "yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
    "LABELS_URL = \"https://raw.githubusercontent.com/tensorflow/models/master/research/audioset/yamnet/yamnet_class_map.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(url):\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        lines = f.read().decode('utf-8').splitlines()\n",
    "    return [line.split(',')[2] for line in lines][1:]  # Extract class names\n",
    "\n",
    "class_labels = load_labels(LABELS_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Audio Processing ----\n",
    "def preprocess_audio(file_path, target_sr=16000):\n",
    "    waveform, sr = librosa.load(file_path, sr=target_sr)\n",
    "    waveform = waveform / np.max(np.abs(waveform))  # Normalize\n",
    "    return waveform, sr\n",
    "\n",
    "def extract_features(waveform, sr=16000, n_mels=128, target_width=128):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=waveform, sr=sr, n_mels=n_mels, fmax=8000)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    \n",
    "    current_width = log_mel_spectrogram.shape[-1]\n",
    "    if current_width < target_width:\n",
    "        pad_width = target_width - current_width\n",
    "        log_mel_spectrogram = np.pad(log_mel_spectrogram, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    elif current_width > target_width:\n",
    "        log_mel_spectrogram = log_mel_spectrogram[:, :target_width]\n",
    "    \n",
    "    return log_mel_spectrogram\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load Data ----\n",
    "def load_data(dataset_folder, set_a_csv, set_b_csv):\n",
    "    df_a = pd.read_csv(set_a_csv)\n",
    "    df_b = pd.read_csv(set_b_csv)\n",
    "    \n",
    "    audio_files = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in df_a.iterrows():\n",
    "        if 'filename' in df_a.columns:\n",
    "            file_path = os.path.join(dataset_folder, 'set_a', row['filename'])\n",
    "            if os.path.exists(file_path):\n",
    "                audio_files.append(file_path)\n",
    "                labels.append(row['label'])\n",
    "    \n",
    "    for _, row in df_b.iterrows():\n",
    "        if 'filename' in df_b.columns:\n",
    "            file_path = os.path.join(dataset_folder, 'set_b', row['filename'])\n",
    "            if os.path.exists(file_path):\n",
    "                audio_files.append(file_path)\n",
    "                labels.append(row['label'])\n",
    "    \n",
    "    return audio_files, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Custom Dataset Class ----\n",
    "class HeartbeatDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, audio_files, labels, batch_size=32, target_sr=16000, shuffle=True):\n",
    "        self.audio_files = audio_files\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.target_sr = target_sr\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.audio_files))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.audio_files) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_audio_files = [self.audio_files[k] for k in batch_indexes]\n",
    "        batch_labels = [self.labels[k] for k in batch_indexes]\n",
    "        X, y = self.__data_generation(batch_audio_files, batch_labels)\n",
    "        return X, y\n",
    "    \n",
    "    def __data_generation(self, batch_audio_files, batch_labels):\n",
    "        batch_labels = np.array(batch_labels)\n",
    "        X = np.empty((self.batch_size, 128, 128))\n",
    "        y = np.empty((self.batch_size, batch_labels.shape[1]))\n",
    "        \n",
    "        for i, audio_file in enumerate(batch_audio_files):\n",
    "            waveform, sr = preprocess_audio(audio_file, self.target_sr)\n",
    "            features = extract_features(waveform, sr)\n",
    "            X[i,] = features\n",
    "            y[i,] = batch_labels[i]\n",
    "        \n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_audio_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m heartbeat_model \u001b[38;5;241m=\u001b[39m train_heartbeat_model(train_audio_files, train_labels, val_audio_files, val_labels)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m     33\u001b[0m heartbeat_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheartbeat_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Saves in HDF5 format\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_audio_files' is not defined"
     ]
    }
   ],
   "source": [
    "# ---- Train Model ----\n",
    "def train_heartbeat_model(train_audio_files, train_labels, val_audio_files, val_labels):\n",
    "    train_labels = np.array(train_labels)\n",
    "    val_labels = np.array(val_labels)\n",
    "    \n",
    "    num_classes = len(set(train_labels))\n",
    "    train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=num_classes)\n",
    "    val_labels = tf.keras.utils.to_categorical(val_labels, num_classes=num_classes)\n",
    "    \n",
    "    train_dataset = HeartbeatDataset(train_audio_files, train_labels, batch_size=32, shuffle=True)\n",
    "    val_dataset = HeartbeatDataset(val_audio_files, val_labels, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(128, 128, 1)),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=10)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_heartbeat_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheartbeat_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Saves in HDF5 format\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "train_heartbeat_model.save(\"heartbeat_model.h5\")  # Saves in HDF5 format\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Example Usage ----\n",
    "dataset_folder = r\"C:\\Users\\ronit\\Desktop\\desktop\\projects\\health prediction\\heart\"\n",
    "set_a_csv = r\"C:\\Users\\ronit\\Desktop\\desktop\\projects\\health prediction\\heart\\set_a_timing.csv\"\n",
    "set_b_csv = r\"C:\\Users\\ronit\\Desktop\\desktop\\projects\\health prediction\\heart\\set_b.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "audio_files, labels = load_data(dataset_folder, set_a_csv, set_b_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio files loaded: 0\n",
      "Total labels loaded: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total audio files loaded: {len(audio_files)}\")\n",
    "print(f\"Total labels loaded: {len(labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No audio files found. Check dataset structure.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(audio_files) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo audio files found. Check dataset structure.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: No audio files found. Check dataset structure."
     ]
    }
   ],
   "source": [
    "if len(audio_files) == 0:\n",
    "    raise ValueError(\"No audio files found. Check dataset structure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split into training and validation sets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_audio_files, val_audio_files, train_labels, val_labels \u001b[38;5;241m=\u001b[39m train_test_split(audio_files, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ronit\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ronit\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2660\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2657\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2659\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2660\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2661\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2662\u001b[0m )\n\u001b[0;32m   2664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ronit\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2308\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2305\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2312\u001b[0m     )\n\u001b[0;32m   2314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Split into training and validation sets\n",
    "train_audio_files, val_audio_files, train_labels, val_labels = train_test_split(audio_files, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio files loaded: 0\n",
      "Total labels loaded: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total audio files loaded: {len(audio_files)}\")\n",
    "print(f\"Total labels loaded: {len(labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            fname  cycle sound  location\n",
      "0  set_a/normal__201102081321.wav      1    S1     10021\n",
      "1  set_a/normal__201102081321.wav      1    S2     20759\n",
      "2  set_a/normal__201102081321.wav      2    S1     35075\n",
      "3  set_a/normal__201102081321.wav      2    S2     47244\n",
      "4  set_a/normal__201102081321.wav      3    S1     62992\n",
      "  dataset                                              fname       label  \\\n",
      "0       b  set_b/Btraining_extrastole_127_1306764300147_C...  extrastole   \n",
      "1       b  set_b/Btraining_extrastole_128_1306344005749_A...  extrastole   \n",
      "2       b  set_b/Btraining_extrastole_130_1306347376079_D...  extrastole   \n",
      "3       b  set_b/Btraining_extrastole_134_1306428161797_C...  extrastole   \n",
      "4       b  set_b/Btraining_extrastole_138_1306762146980_B...  extrastole   \n",
      "\n",
      "  sublabel  \n",
      "0      NaN  \n",
      "1      NaN  \n",
      "2      NaN  \n",
      "3      NaN  \n",
      "4      NaN  \n"
     ]
    }
   ],
   "source": [
    "set_a_df = pd.read_csv(set_a_csv)\n",
    "set_b_df = pd.read_csv(set_b_csv)\n",
    "print(set_a_df.head())  # Ensure 'filename' column exists\n",
    "print(set_b_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m data_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      2\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_folder, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Adjust if needed\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "for _, row in data_df.iterrows():\n",
    "    file_path = os.path.join(dataset_folder, row['filename'])  # Adjust if needed\n",
    "    print(f\"Checking file: {file_path}\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "heartbeat_model = train_heartbeat_model(train_audio_files, train_labels, val_audio_files, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
